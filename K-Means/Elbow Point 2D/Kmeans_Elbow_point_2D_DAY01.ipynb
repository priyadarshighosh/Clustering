{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Clustering K means CLustering\n"
      ],
      "metadata": {
        "id": "buV9E0bf-37g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING THE DATA SCIENCE LIBRARIES"
      ],
      "metadata": {
        "id": "HNvHInaq_BMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eum2WGOnBeeL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the ML / Scikit learn EVERYTHING"
      ],
      "metadata": {
        "id": "egI1aKHM_G4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "rw9qEruauW9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split               #for splitting the data into test and training data\n",
        "from sklearn.compose import ColumnTransformer                       #for transforming the columns\n",
        "from sklearn.impute import SimpleImputer                             #for imputing the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder                      #one hot encoding\n",
        "from sklearn.preprocessing import MinMaxScaler                        #standard scaling\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score                 # for accuracy score\n",
        "from sklearn.model_selection import cross_val_score        # for cross validation score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression           # Import the LinearRegression class\n",
        "from sklearn.metrics import mean_squared_error, r2_score    # to find out the error functions\n",
        "from sklearn.preprocessing import PolynomialFeatures , StandardScaler   # for the polunomial features\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge   # ridge Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier                 #Decision tree\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier   # bagging and Boosting\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score   # different metrices to check how our model performed"
      ],
      "metadata": {
        "id": "wRReSbVmlxcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Deep learning Everything"
      ],
      "metadata": {
        "id": "7EOOII0y82hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "O1qjFngb87Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing mathematical models"
      ],
      "metadata": {
        "id": "5oPnYaeAM0_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import the mathematical testing framewroks\n",
        "\n",
        "# Importing mathematical models\n",
        "import math\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "G_2PyN5pNV1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Information about the Dataset"
      ],
      "metadata": {
        "id": "XTPRT3iy9EuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files    # we are importing the file from the device\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "GiKUuK2LUcY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical encoding has already being performed"
      ],
      "metadata": {
        "id": "u9tMczjh9x1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('student_clustering.csv')   #fitting the data in the df dataframe\n",
        "df.head(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "PAXRZRNaJVKO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "fkHEM3gG747n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "tI7KmcrM79Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()       # we are checking for the null values luckily we dont have any here"
      ],
      "metadata": {
        "id": "i0p4tkGa8LcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " There is no null data present in our Dataset"
      ],
      "metadata": {
        "id": "wUxiZbKg8QuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "YP84BY9b8Zdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()          # Checking the Duplicate Columns"
      ],
      "metadata": {
        "id": "iUBfdUMF8dqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard Scaling the numeric values"
      ],
      "metadata": {
        "id": "p7-0H-rSAg7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "HA-b0z5ClJzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns =  ['cgpa' ,'iq']\n",
        "\n",
        "\n",
        "def remove_outliers(df , columns ):       # Outlier detection of the Numeric columns\n",
        "  for col in columns:\n",
        "\n",
        "    Q1 = df[col].quantile(0.25)            # 25th percentile\n",
        "    Q3 = df[col].quantile(0.75)               # 75th percentile\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR           # lower bound and upper bound a little less than 25th percentile and little more than 75th percentile\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "\n",
        "  return df\n",
        "\n",
        "df = remove_outliers(df , numerical_columns)"
      ],
      "metadata": {
        "id": "XePWWals8sqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "KND3ot6zltuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns =  ['cgpa','iq']"
      ],
      "metadata": {
        "id": "J9NXMB508jG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])"
      ],
      "metadata": {
        "id": "F-OkP875As0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "N0rDH33Tl8Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using Inter-quartile Range to find the ouliers in the Numerical Continuous Values of the Numerical columns that we have"
      ],
      "metadata": {
        "id": "hLGjtdxj-gIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLotting the Graph for Points of CGPA and IQ"
      ],
      "metadata": {
        "id": "IOeYbyzlySTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import  matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.scatter(df['cgpa'],df['iq'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2hmCTVs-rue8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate the WCSS for the Points"
      ],
      "metadata": {
        "id": "tUSyycdLxXCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "E6B341teuJr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WCSS = []\n",
        "                                                  #Getting the WCSS till 11 points\n",
        "for i in range(1,11):\n",
        "  km = KMeans(n_clusters = i)\n",
        "  km.fit_predict(df)\n",
        "  WCSS.append(km.inertia_)"
      ],
      "metadata": {
        "id": "i_lDlIVtuU4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WCSS"
      ],
      "metadata": {
        "id": "njN_nbfNxG_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,11),WCSS)\n",
        "plt.title('The Elbow Method')                    # Plotting the WCSS with the help of Elbow Method\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('WCSS')"
      ],
      "metadata": {
        "id": "2QbGAXCex3Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4 is the ELbow Point , value  of K is 4"
      ],
      "metadata": {
        "id": "DQRuOODBx9Kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the K-Means Custering"
      ],
      "metadata": {
        "id": "oSQRCUxryY2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X  = df.iloc[:,:].values\n",
        "km  = KMeans(n_clusters = 4)\n",
        "y_means = km.fit_predict(X)\n"
      ],
      "metadata": {
        "id": "PYt8zS7-yqoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_means"
      ],
      "metadata": {
        "id": "KCci_r8DzGy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WE have 4 clusters 0 , 1 , 2 and 3"
      ],
      "metadata": {
        "id": "63w2RLqrzqig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[y_means == 0 ]    # Rows that come in Custer 0"
      ],
      "metadata": {
        "id": "RRONuRuozKuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[y_means == 1 ]    # Rows that come in Custer 1"
      ],
      "metadata": {
        "id": "vJFIAl6pzZB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[y_means == 2 ]    # Rows that come in Custer 2"
      ],
      "metadata": {
        "id": "jNrMjiTNz3D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[y_means == 3 ]    # Rows that come in Custer 3"
      ],
      "metadata": {
        "id": "KBT9HIjc0BvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[y_means == 0,0], X[y_means == 0,1 ],color ='blue')\n",
        "plt.scatter(X[y_means == 1,0], X[y_means == 1,1 ],color ='red')\n",
        "plt.scatter(X[y_means == 2,0], X[y_means == 2,1 ],color ='green')\n",
        "plt.scatter(X[y_means == 3,0], X[y_means == 3,1 ],color ='yellow')"
      ],
      "metadata": {
        "id": "GBn2pq1H0Eu7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}